{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fourroomsEnv\n",
    "import option\n",
    "import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ac95ab4b8a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'''This code does a Monte Carlo return with n agents with common belief estimated as mixGauss'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMCR_Gauss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mean_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cov_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "'''This code does a Monte Carlo return with n agents with common belief estimated as mixGauss'''\n",
    "\n",
    "class MCR_Gauss(env):\n",
    "    \n",
    "    def __init__(self, n_agents, n_mean_prior, n_cov_prior, maxIter):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_mean_prior = n_mean_prior\n",
    "        self.n_cov_prior = n_cov_prior\n",
    "        self.maxIter = maxIter\n",
    "        super(MCR_Gauss, self).__init__()\n",
    "        \n",
    "        \n",
    "    def inverseNW(self, mean_prior, cov_prior, samples, n_sample):\n",
    "        '''Inverse Normal-Wishart for estimating multi-variate normal'''\n",
    "        '''samples is a list of lists (for multiple samples), or a list (for one sample)'''\n",
    "\n",
    "        x = np.array(samples).T #alternatively: np.transpose(samples)\n",
    "        C = np.cov(x) #sample covariance\n",
    "        barX = np.mean(x, axis = 0) #sample mean\n",
    "\n",
    "        mean_posterior = (self.n_mean_prior*mean_prior + n_sample*barX)/(self.n_mean_prior+n_sample)\n",
    "\n",
    "        Psi = self.n_cov_prior*cov_prior\n",
    "\n",
    "        inter = ((self.n_mean_prior*n_sample)/(self.n_mean_prior+n_sample))*(barX-mean_prior)*np.transpose(barX-mean_prior)\n",
    "\n",
    "        cov_posterior = C + Psi + inter\n",
    "\n",
    "        return (mean_posterior, (self.n_mean_prior+n_sample), (self.n_cov_prior+n_sample), cov_posterior)\n",
    "        \n",
    "    \n",
    "    def priors(self):\n",
    "        #randomly choose a prior mean\n",
    "        mean_prior_samples = np.random.multivariate_normal([0,0], np.eye(self.n_agents), self.n_mean_prior) #alternatively, just choose mean_prior = [0,0]\n",
    "        mean_prior = np.mean(mean_prior_samples,axis = 0)\n",
    "\n",
    "        sigma_samples = [np.random.uniform(0, 4, self.n_cov_prior) for _ in range(self.n_agents)]\n",
    "        avg_sigmas = np.mean(sigma_samples,axis = 1)\n",
    "        cov_prior = np.diag(avg_sigmas)\n",
    "        return (mean_prior, cov_prior)\n",
    "    \n",
    "    \n",
    "    def gaussianUpdate(self, mean_prior, cov_prior, data):\n",
    "        (mean_posterior, n_mean_posterior, n_cov_posterior, cov_posterior) = self.inverseNW(mean_prior, cov_prior, data, 1) #update based on one sample joint-observation\n",
    "        return (mean_posterior, n_mean_posterior, n_cov_posterior, cov_posterior)\n",
    "    \n",
    "    \n",
    "    def sampleFromPosterior(self, mean_prior, cov_prior, data, n_samples):\n",
    "        return np.random.multivariate_normal(self.gaussianUpdate(mean_prior, cov_prior, data)[0], self.gaussianUpdate(mean_prior, cov_prior, data)[3], n_samples)\n",
    "\n",
    "    \n",
    "    def reward(self, joint_state, joint_action):  # dummy reward. We have to modify the environment to incorporate the reward based on the sampled next state\n",
    "        return np.linalg.norm(state)+np.linalg.norm(joint_action)\n",
    "    \n",
    "    def mc_episode(self, mean_prior, cov_prior, features, policy, n_episodes, n_steps): \n",
    "        for ep in range(n_episodes):\n",
    "            episode = []\n",
    "            Q_inter = defaultdict(list)\n",
    "            \n",
    "            phi = features(env.reset())\n",
    "\n",
    "            joint_action = [policy.sample(phi) for _ in range(self.n_agents)] \n",
    "            \n",
    "            common_observation = env.get_observation(Option.broadcasts)\n",
    "\n",
    "            mean_prior, cov_prior = self.priors()\n",
    "\n",
    "            \n",
    "            for step in range(n_steps):\n",
    "                common_observation = env.get_observation(Option.broadcasts)\n",
    "\n",
    "                # Note that for a given joint-action the reward, next joint-state should be coded in env.step()\n",
    "                common_belief_mean_posterior = self.gaussianUpdate(mean_prior, cov_prior, common_observation)[0]\n",
    "                common_belief_cov_posterior = self.gaussianUpdate(mean_prior, cov_prior,common_observation)[3]\n",
    "\n",
    "\n",
    "                next_joint_state = np.mean(self.sampleFromPosterior(100), axis = 0)\n",
    "                phi = features(next_joint_state)\n",
    "                joint_action = [pi_policy().sample(phi) for _ in range(args.n_agents)]\n",
    "                reward = self.reward(next_joint_state, joint_action)\n",
    "\n",
    "                samples_joint_states = np.random.multivariate_normal(mean_prior, cov_prior, 100)\n",
    "                joint_state = np.mean(samples_joint_states, axis = 0) #sample_average for estimated joint-state\n",
    "\n",
    "                episode.append([joint_state, joint_action, reward])\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                mean_prior = common_belief_mean_posterior\n",
    "                cov_prior = common_belief_cov_posterior\n",
    "\n",
    "                reward_from_episode = np.zeros([env.observation_space.n,env.action_space.n]) #target reward from episode\n",
    "                for t1 in reversed(range(len(episode))):\n",
    "                    joint_state, joint_action, reward = episode[t1]\n",
    "\n",
    "                    reward_from_episode[joint_state,joint_action] = args.discount*reward_from_episode[joint_state,joint_action] + reward\n",
    "                    Q_inter[joint_state,joint_action].append(reward_from_episode[joint_state,joint_action]) #every visit MC\n",
    "                saEpisode = set([(x[0], x[1]) for x in episode]) \n",
    "\n",
    "                for (s,a) in saEpisode:\n",
    "                    Q[s,a] = np.mean(Q_inter[(s,a)])\n",
    "        return Q\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class Tabular:\n",
    "    def __init__(self, n_states):\n",
    "        self.n_states = n_states\n",
    "\n",
    "    def __call__(self, state): #state is a tuple having n_agents elements\n",
    "        return np.array([state,])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_states\n",
    "    \n",
    "\n",
    "    \n",
    "class SoftmaxPolicy:\n",
    "    def __init__(self, rng, n_features, n_actions, temp=1.):\n",
    "        self.rng = rng\n",
    "        self.weights = np.zeros((n_states, n_actions)) # we assume that n_features and n_actions for all agents are same\n",
    "        self.temp = temp\n",
    "\n",
    "    def value(self, phi, agent_action=None): #Phi is a n_features x numAgents dimensional matrix, joint_action is a list \n",
    "        if agent_action is None:\n",
    "            v = np.sum(self.weights[phi,:], axis=0)\n",
    "        v = np.sum(self.weights[phi, agent_action], axis=0)\n",
    "        return v\n",
    "\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        v = self.value(agent_state)/self.temp\n",
    "        pmf = np.exp(v - logsumexp(v))\n",
    "        return pmf\n",
    "\n",
    "\n",
    "    def sample(self, phi):\n",
    "        return int(self.rng.choice(self.weights.shape[1], p=self.pmf(phi)))\n",
    "    \n",
    "    \n",
    "class EgreedyPolicy:\n",
    "    def __init__(self, numAgents, rng, n_features, n_actions, epsilon):\n",
    "        self.numAgents = numAgents\n",
    "        self.rng = rng\n",
    "        self.epsilon = epsilon\n",
    "        self.weights = np.zeros((n_features, n_actions))\n",
    "\n",
    "    def value(self, phi, agent_action=None):\n",
    "        if agent_action is None:\n",
    "            v = np.sum(self.weights[phi, :], axis=0)\n",
    "        v = np.sum(self.weights[phi, agent_action], axis=0)\n",
    "        return v\n",
    "\n",
    "    def sample(self, phi):\n",
    "        if self.rng.uniform() < self.epsilon:\n",
    "            agent_action = int(self.rng.randint(self.weights.shape[1]))\n",
    "        agent_action = int(np.argmax(self.value(phi))) \n",
    "        return agent_action\n",
    "\n",
    "    \n",
    "    #____________________________ Main algo _______________________\n",
    "    \n",
    "    \n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--n_agents', help='Number of agents', type=int, default=1)\n",
    "        parser.add_argument('--n_goals', help='Number of goals', type=int, default=1)\n",
    "        parser.add_argument('--discount', help='Discount factor', type=float, default=0.99)\n",
    "        parser.add_argument('--lr_term', help=\"Termination gradient learning rate\", type=float, default=1e-3)\n",
    "        parser.add_argument('--lr_intra', help=\"Intra-option gradient learning rate\", type=float, default=1e-3)\n",
    "        parser.add_argument('--lr_critic', help=\"Learning rate\", type=float, default=1e-2)\n",
    "        parser.add_argument('--epsilon', help=\"Epsilon-greedy for policy over options\", type=float, default=1e-2)\n",
    "        parser.add_argument('--n_episodes', help=\"Number of episodes per run\", type=int, default=250)\n",
    "        parser.add_argument('--n_runs', help=\"Number of runs\", type=int, default=100)\n",
    "        parser.add_argument('--n_steps', help=\"Maximum number of steps per episode\", type=int, default=1000)\n",
    "        parser.add_argument('--n_mean_prior', help=\"Number of samples to compute mean vector of Gaussian common belief\", type=int, default=100)\n",
    "        parser.add_argument('--n_cov_prior', help=\"Number of samples to compute covariance matrix of Gaussian common belief\", type=int, default=100)\n",
    "        parser.add_argument('--temperature', help=\"Temperature parameter for softmax\", type=float, default=1e-2)\n",
    "        \n",
    "        \n",
    "        args = parser.parse_args()\n",
    "\n",
    "        rng = np.random.RandomState(1234)\n",
    "        env = gym.make('FourroomsEnv-v0')\n",
    "        \n",
    "        f_name = '-'.join(['{}_{}'.format(param, val) for param, val in sorted(vars(args).items())])\n",
    "        f_name = 'commonBeliefGaussian-mcReturn-fourroomsEnv-' + f_name + '.npy'\n",
    "        \n",
    "        possible_next_goals = [68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103]\n",
    "\n",
    "        \n",
    "        #traces = np.zeros((args.n_runs, args.n_episodes, args.n_agents+1))\n",
    "        traces = {k:(0,np.zeros([env.observation_space.n,env.action_space.n])) for k in range(args.n_runs)}\n",
    "        \n",
    "        for run in range(args.n_runs):\n",
    "            features = Tabular(env.observation_space.n)\n",
    "            n_features, nactions = len(features), env.action_space.n #len(features) = 1\n",
    "            \n",
    "            common_belief_mean_prior, common_belief_cov_prior = MCR_Gauss(env, args.n_agents, args.n_mean_prior, args.n_cov_prior, maxIter).priors()\n",
    "            pi_policy = [SoftmaxPolicy(rng, n_features, args.n_actions) for _ in range(args.n_agents)]\n",
    "            \n",
    "            \n",
    "            broadcasts = [Option().broadcast for _ in range(args.n_agents)]   \n",
    "        \n",
    "                \n",
    "            #Compute the Monte Carlo return of episodes\n",
    "            Q = MCR_Gauss(env, args.n_agents, args.n_mean_prior, args.n_cov_prior, args.n_steps).mc_episode(mean_prior, cov_prior, features(env.reset()), SoftmaxPolicy(rng, n_features, n_actions), args.n_episodes, args.n_steps)\n",
    "            traces[run] = (step, Q)    \n",
    "                    \n",
    "            print('Run {} episode {} steps {} Q {}'.format(run, episode, step, Q))\n",
    "        np.save(f_name, traces)\n",
    "        print(f_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
