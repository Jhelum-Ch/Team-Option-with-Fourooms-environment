{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "\n",
    "from distributed.belief import MultinomialDirichletBelief\n",
    "from distributed.broadcast import Broadcast\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DOC:\n",
    "    def __init__(self, env, options, mu_policy):\n",
    "        '''\n",
    "        :param states_list: all combination of joint states. This is an input from the environment\n",
    "        :param lr_thea: list of learning rates for learning policy parameters (pi), for all the agents\n",
    "        :param lr_phi: list of learning rates for learning termination functions (beta), for all the agents\n",
    "        :param init_observation: list of joint observation of all the agents\n",
    "        '''\n",
    "\n",
    "        self.env = env\n",
    "        self.options = options\n",
    "    \n",
    "        '''\n",
    "        2. Start with initial common belief b\n",
    "        '''\n",
    "        # set initial belief\n",
    "        initial_joint_observation = params['env']['initial_joint_state']\n",
    "        self.b = MultinomialDirichletBelief(env, initial_joint_observation)\n",
    "        #self.b0 = Belief(env)\n",
    "\n",
    "        '''\n",
    "        3. Sample a joint state s := vec(s_1,...,s_n) according to b_0\n",
    "        '''\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # policy over options\n",
    "        self.mu_policy = mu_policy\n",
    "        print(self.mu_policy)\n",
    "\n",
    "        self.o = self.mu_policy.sample(self.s) #self.chooseOption()\n",
    "        self.a = self.chooseAction()\n",
    "\n",
    "        \n",
    "    #def chooseOption(self):\n",
    "    \n",
    "#         # Choose joint-option o based on softmax option-policy\n",
    "        \n",
    "#         #select agents randomly to pick options\n",
    "#         agent_order = [agent.ID for agent in self.env.agents]\n",
    "#         shuffle(agent_order)\n",
    "#         print('agent order :', agent_order)\n",
    "    \n",
    "#         #let agents select options from available option pool\n",
    "#         for agent in agent_order:\n",
    "#             option_mask = [not(option.available) for option in self.options]\n",
    "#             # print(option_mask)\n",
    "    \n",
    "#             # pmf = [0, 0, 0.7, 0.1, 0.2]\n",
    "#             pmf = self.mu_policy.pmf(self.s[agent])\n",
    "#             pmf = np.ma.masked_array(pmf, option_mask)\n",
    "#             # print('pmf : ', pmf)\n",
    "\n",
    "#             # select option for agent\n",
    "#             # TODO : in order to sample option instead of choosing the best one, the masked pdf needs to be re-normalized\n",
    "#             selected_option_idx = np.argmax(pmf)\n",
    "#             self.env.agents[agent].option = self.options[selected_option_idx].optionID\n",
    "#             # print(selected_option_idx)\n",
    "\n",
    "#             #remove the selected option from available option pool by setting availability to False\n",
    "#             self.options[selected_option_idx].available = False\n",
    "\n",
    "    def chooseAction(self):\n",
    "        joint_action = []\n",
    "        for agent in self.env.agents:\n",
    "            action = self.options[agent.option].policy.sample(agent.state)\n",
    "            print('agent ID:', agent.ID, 'state:', agent.state, 'option ID:', agent.option, 'action:', action)\n",
    "            agent.action = action\n",
    "            joint_action.append(action)\n",
    "\n",
    "        return joint_action\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def evaluateOption(self, critic, action_critic, terminations, baseline=False):\n",
    "        # critic.start(joint_state, joint_option)\n",
    "        # action_critic.start(joint_state, joint_option, joint_action)\n",
    "        \n",
    "        reward, next_true_joint_state, done, _ = self.env.step(joint_action)\n",
    "\n",
    "        broadcasts = Broadcast(self.env, next_true_joint_state, self.s, self.o, terminations)\n",
    "\n",
    "        #broadcasts = self.env.broadcast(reward, next_true_joint_state, self.s, self.o, terminations)\n",
    "        joint_observation = self.env.get_observation(broadcasts)\n",
    "\n",
    "        self.b = MultinomialDirichletBelief(self.env, joint_observation)\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # Critic update\n",
    "        update_target = critic.update(self.s, self.o, reward, done)\n",
    "        action_critic.update(self.s, self.o, self.a, reward, done)\n",
    "\n",
    "\n",
    "        critic_feedback = action_critic.getQvalue(self.s, self.o, self.a)  #Q(s,o,a)\n",
    "\n",
    "        if baseline:\n",
    "            critic_feedback -= critic.value(self.s, self.o)\n",
    "        return critic_feedback\n",
    "\n",
    "\n",
    "    def improveOption_of_agent(self, agentID, intra_option_policy_improvement, termination_improvement, critic_feedback):\n",
    "        return intra_option_policy_improvement.update(agent_state, agent_action, critic_feedback), termination_improvement.update(agentID, self.s, self.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <optionCritic.policies.SoftmaxActionPolicy object at 0x113065518> <optionCritic.termination.SigmoidTermination object at 0x113065470> True\n",
      "<optionCritic.policies.SoftmaxOptionPolicy object at 0x113065128>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f3da97774f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDOC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavail_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'agent option'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0492372a2680>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, options, mu_policy)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#self.chooseOption()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchooseAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-0492372a2680>\u001b[0m in \u001b[0;36mchooseAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mjoint_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agent ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'option ID:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'action:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not NoneType"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from fourroomsEnv import FourroomsMA\n",
    "from modelConfig import params\n",
    "from optionCritic.option import Option, createOptions\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "from optionCritic.termination import SigmoidTermination\n",
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "import optionCritic.gradients as grads\n",
    "\n",
    "env = FourroomsMA()\n",
    "avail_options, mu_policies = createOptions(env)\n",
    "#print(avail_options[0].optionID, avail_options[0].policy, avail_options[0].termination, avail_options[0].available)\n",
    "\n",
    "joint_state_list = set([tuple(np.sort(s)) for s in env.states_list])\n",
    "joint_option_list = list(itertools.permutations(range(params['agent']['n_options']), params['env']['n_agents']))\n",
    "joint_action_list = list(itertools.product(range(len(env.agent_actions)), repeat=params['env']['n_agents']))\n",
    "\n",
    "# mu_policy is the policy over options\n",
    "mu_weights = dict.fromkeys(joint_state_list, dict.fromkeys(joint_option_list, 0))\n",
    "mu_policy = SoftmaxOptionPolicy(mu_weights)\n",
    "\n",
    "\n",
    "pi_policies = [SoftmaxActionPolicy(len(env.cell_list), len(env.agent_actions)) for _ in avail_options]\n",
    "\n",
    "# terminations take agent's state (not joint-state)\n",
    "option_terminations = [SigmoidTermination(len(env.cell_list)) for _ in range(params['agent']['n_options'])]\n",
    "critic = IntraOptionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations)\n",
    "\n",
    "\n",
    "action_critic = IntraOptionActionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations, critic)\n",
    "\n",
    "\n",
    "agent = DOC(env, avail_options, mu_policies).env.agents[0]\n",
    "print('agent',agent, 'agent option', agent.option)\n",
    "\n",
    "pi_policy_of_agent_option = pi_policies[agent.option]\n",
    "agent_option_termination = option_terminations[agent.option]\n",
    "\n",
    "intra_option_policy_improvement = grads.IntraOptionGradient(pi_policy_of_agent_option, params['doc']['lr_theta'])\n",
    "termination_improvement = grads.TerminationGradient(agent_option_termination, critic, params['doc']['lr_phi'])\n",
    "\n",
    "\n",
    "joint_state = DOC(env, avail_options, mu_policies).s\n",
    "joint_option = DOC(env, avail_options, mu_policies).o\n",
    "print('jointoption', joint_option)\n",
    "joint_action = DOC(env, avail_options, mu_policies).a\n",
    "\n",
    "# agent_state = joint_state[agentID]\n",
    "# agent_option = joint_option[agentID]\n",
    "# agent_action = joint_action[agentID]\n",
    "agent_state = agent.state\n",
    "agent_option = agent.option\n",
    "agent_action = agent.action\n",
    "\n",
    "critic.start(joint_state, joint_option)\n",
    "action_critic.start(joint_state, joint_option, joint_action)\n",
    "\n",
    "evalOption = DOC(env, avail_options, mu_policies).evaluateOption(critic, action_critic, option_terminations, baseline=False)\n",
    "imprvOption = DOC(env, avail_options, mu_policies).improveOption_of_agent(agent.ID, intra_option_policy_improvement, termination_improvement, evalOption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
