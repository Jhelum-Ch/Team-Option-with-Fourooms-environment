{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fourroomsEnv\n",
    "import option\n",
    "import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This code does a Monte Carlo return with n agents with common belief estimated as mixGauss'''\n",
    "\n",
    "class MCR_Gauss(env):\n",
    "    \n",
    "    def __init__(self, n_agents, n_mean_prior, n_cov_prior, maxIter):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_mean_prior = n_mean_prior\n",
    "        self.n_cov_prior = n_cov_prior\n",
    "        self.maxIter = maxIter\n",
    "        super(MCR_Gauss, self).__init__()\n",
    "        \n",
    "        \n",
    "    def inverseNW(self, mean_prior, cov_prior, samples, n_sample):\n",
    "        '''Inverse Normal-Wishart for estimating multi-variate normal'''\n",
    "        '''samples is a list of lists (for multiple samples), or a list (for one sample)'''\n",
    "\n",
    "        x = np.array(samples).T #alternatively: np.transpose(samples)\n",
    "        C = np.cov(x) #sample covariance\n",
    "        barX = np.mean(x, axis = 0) #sample mean\n",
    "\n",
    "        mean_posterior = (self.n_mean_prior*mean_prior + n_sample*barX)/(self.n_mean_prior+n_sample)\n",
    "\n",
    "        Psi = self.n_cov_prior*cov_prior\n",
    "\n",
    "        inter = ((self.n_mean_prior*n_sample)/(self.n_mean_prior+n_sample))*(barX-mean_prior)*np.transpose(barX-mean_prior)\n",
    "\n",
    "        cov_posterior = C + Psi + inter\n",
    "\n",
    "        return (mean_posterior, (self.n_mean_prior+n_sample), (self.n_cov_prior+n_sample), cov_posterior)\n",
    "        \n",
    "    \n",
    "    def priors(self):\n",
    "        #randomly choose a prior mean\n",
    "        mean_prior_samples = np.random.multivariate_normal([0,0], np.eye(self.n_agents), self.n_mean_prior) #alternatively, just choose mean_prior = [0,0]\n",
    "        mean_prior = np.mean(mean_prior_samples,axis = 0)\n",
    "\n",
    "        sigma_samples = [np.random.uniform(0, 4, self.n_cov_prior) for _ in range(self.n_agents)]\n",
    "        avg_sigmas = np.mean(sigma_samples,axis = 1)\n",
    "        cov_prior = np.diag(avg_sigmas)\n",
    "        return (mean_prior, cov_prior)\n",
    "    \n",
    "    \n",
    "    def gaussianUpdate(self, mean_prior, cov_prior, data):\n",
    "        (mean_posterior, n_mean_posterior, n_cov_posterior, cov_posterior) = self.inverseNW(mean_prior, cov_prior, data, 1) #update based on one sample joint-observation\n",
    "        return (mean_posterior, n_mean_posterior, n_cov_posterior, cov_posterior)\n",
    "    \n",
    "    \n",
    "    def sampleFromPosterior(self, mean_prior, cov_prior, data, n_samples):\n",
    "        return np.random.multivariate_normal(self.gaussianUpdate(mean_prior, cov_prior, data)[0], self.gaussianUpdate(mean_prior, cov_prior, data)[3], n_samples)\n",
    "\n",
    "    \n",
    "    def reward(self, joint_state, joint_action):  # dummy reward. We have to modify the environment to incorporate the reward based on the sampled next state\n",
    "        return np.linalg.norm(state)+np.linalg.norm(joint_action)\n",
    "    \n",
    "    def mc_episode(self, mean_prior, cov_prior, features, n_episodes, n_steps):\n",
    "        for ep in range(n_episodes):\n",
    "            episode = []\n",
    "            Q_inter = defaultdict(list)\n",
    "\n",
    "            phi = features(env.reset())\n",
    "\n",
    "            joint_action = [pi_policy().sample(phi) for _ in range(self.n_agents)] \n",
    "            \n",
    "            # define broadcasts \n",
    "            common_observation = env.get_observation(broadcasts)\n",
    "\n",
    "            mean_prior, cov_prior = self.priors()\n",
    "\n",
    "            \n",
    "            for step in range(n_steps):\n",
    "                # define broadcasts  \n",
    "                common_observation = env.get_observation(broadcasts)\n",
    "\n",
    "                # Note that for a given joint-action the reward, next joint-state should be codede in env.step()\n",
    "                common_belief_mean_posterior = self.gaussianUpdate(mean_prior, cov_prior, common_observation)[0]\n",
    "                common_belief_cov_posterior = self.gaussianUpdate(mean_prior, cov_prior,common_observation)[3]\n",
    "\n",
    "\n",
    "                next_joint_state = np.mean(self.sampleFromPosterior(100), axis = 0)\n",
    "                phi = features(next_joint_state)\n",
    "                joint_action = [pi_policy().sample(phi) for _ in range(args.n_agents)]\n",
    "                reward = self.reward(next_joint_state, joint_action)\n",
    "\n",
    "                samples_joint_states = np.random.multivariate_normal(mean_prior, cov_prior, 100)\n",
    "                joint_state = np.mean(samples_joint_states, axis = 0) #sample_average for estimated joint-state\n",
    "\n",
    "                episode.append([joint_state, joint_action, reward])\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                mean_prior = common_belief_mean_posterior\n",
    "                cov_prior = common_belief_cov_posterior\n",
    "\n",
    "                reward_from_episode = np.zeros([env.observation_space.n,env.action_space.n]) #target reward from episode\n",
    "                for t1 in reversed(range(len(episode))):\n",
    "                    joint_state, joint_action, reward = episode[t1]\n",
    "\n",
    "                    reward_from_episode[joint_state,joint_action] = args.discount*reward_from_episode[joint_state,joint_action] + reward\n",
    "                    Q_inter[joint_state,joint_action].append(reward_from_episode[joint_state,joint_action]) #every visit MC\n",
    "                saEpisode = set([(x[0], x[1]) for x in episode]) \n",
    "\n",
    "                for (s,a) in saEpisode:\n",
    "                    Q[s,a] = np.mean(Q_inter[(s,a)])\n",
    "        return Q\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class Tabular:\n",
    "    def __init__(self, n_states):\n",
    "        self.n_states = n_states\n",
    "\n",
    "    def __call__(self, state): #state is a tuple having n_agents elements\n",
    "        return np.array([state,])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_states\n",
    "    \n",
    "\n",
    "    \n",
    "class SoftmaxPolicy:\n",
    "    def __init__(self, rng, n_features, n_actions, temp=1.):\n",
    "        self.rng = rng\n",
    "        self.weights = np.zeros((n_states, n_actions)) # we assume that n_features and n_actions for all agents are same\n",
    "        self.temp = temp\n",
    "\n",
    "    def value(self, phi, agent_action=None): #Phi is a n_features x numAgents dimensional matrix, joint_action is a list \n",
    "         if agent_action is None:\n",
    "                value = np.sum(self.weights[phi,:], axis=0)\n",
    "            value = np.sum(self.weights[phi, agent_action], axis=0)\n",
    "        return value\n",
    "\n",
    "\n",
    "    def pmf(self, phi):\n",
    "        v = self.value(agent_state)/self.temp\n",
    "        pmf = np.exp(v - logsumexp(v))\n",
    "        return pmf\n",
    "\n",
    "\n",
    "    def sample(self, phi):\n",
    "        return int(self.rng.choice(self.weights.shape[1], p=self.pmf(phi)))\n",
    "    \n",
    "    \n",
    "class EgreedyPolicy:\n",
    "    def __init__(self, numAgents, rng, n_features, n_actions, epsilon):\n",
    "        self.numAgents = numAgents\n",
    "        self.rng = rng\n",
    "        self.epsilon = epsilon\n",
    "        self.weights = np.zeros((n_features, n_actions))\n",
    "\n",
    "    def value(self, phi, joint_action=[None for _ in range(self.numAgents)]):\n",
    "        if joint_action[i] is None:\n",
    "            value = np.sum(self.weights[phi, :], axis=0)\n",
    "        value = np.sum(self.weights[phi, joint_action[i]], axis=0)\n",
    "        return value\n",
    "\n",
    "    def sample(self, phi):\n",
    "        if self.rng.uniform() < self.epsilon:\n",
    "            action = int(self.rng.randint(self.weights.shape[1]))\n",
    "        action = int(np.argmax(self.value(phi))) \n",
    "        return action\n",
    "\n",
    "    \n",
    "    #____________________________ Main algo _______________________\n",
    "    \n",
    "    \n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--n_agents', help='Number of agents', type=int, default=1)\n",
    "        parser.add_argument('--n_goals', help='Number of goals', type=int, default=1)\n",
    "        parser.add_argument('--discount', help='Discount factor', type=float, default=0.99)\n",
    "        parser.add_argument('--lr_term', help=\"Termination gradient learning rate\", type=float, default=1e-3)\n",
    "        parser.add_argument('--lr_intra', help=\"Intra-option gradient learning rate\", type=float, default=1e-3)\n",
    "        parser.add_argument('--lr_critic', help=\"Learning rate\", type=float, default=1e-2)\n",
    "        parser.add_argument('--epsilon', help=\"Epsilon-greedy for policy over options\", type=float, default=1e-2)\n",
    "        parser.add_argument('--n_episodes', help=\"Number of episodes per run\", type=int, default=250)\n",
    "        parser.add_argument('--n_runs', help=\"Number of runs\", type=int, default=100)\n",
    "        parser.add_argument('--n_steps', help=\"Maximum number of steps per episode\", type=int, default=1000)\n",
    "        parser.add_argument('--n_mean_prior', help=\"Number of samples to compute mean vector of Gaussian common belief\", type=int, default=100)\n",
    "        parser.add_argument('--n_cov_prior', help=\"Number of samples to compute covariance matrix of Gaussian common belief\", type=int, default=100)\n",
    "        parser.add_argument('--temperature', help=\"Temperature parameter for softmax\", type=float, default=1e-2)\n",
    "        \n",
    "        \n",
    "        args = parser.parse_args()\n",
    "\n",
    "        rng = np.random.RandomState(1234)\n",
    "        env = gym.make('FourroomsEnv-v0')\n",
    "        \n",
    "        f_name = '-'.join(['{}_{}'.format(param, val) for param, val in sorted(vars(args).items())])\n",
    "        f_name = 'optioncritic-fourroomsEnv-' + f_name + '.npy'\n",
    "        \n",
    "        possible_next_goals = [68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103]\n",
    "\n",
    "        \n",
    "        #traces = np.zeros((args.n_runs, args.n_episodes, args.n_agents+1))\n",
    "        traces = {k:(0,np.zeros([env.observation_space.n,env.action_space.n])) for k in range(args.n_runs)}\n",
    "        \n",
    "        for run in range(args.n_runs):\n",
    "            features = Tabular(env.observation_space.n)\n",
    "            n_features, nactions = len(features), env.action_space.n #len(features) = 1\n",
    "            \n",
    "            common_belief_mean_prior, common_belief_cov_prior = MCR_Gauss(env, args.n_agents, args.n_mean_prior, args.n_cov_prior, maxIter).priors()\n",
    "            pi_policy = [SoftmaxPolicy(rng, n_features, args.n_actions) for _ in range(args.n_agents)]\n",
    "            \n",
    "            \n",
    "            broadcasts = [Option().broadcast for _ in range(args.n_agents)]   \n",
    "        \n",
    "                \n",
    "            #Compute the Monte Carlo return of episodes\n",
    "            Q = MCR_Gauss(env, args.n_agents, args.n_mean_prior, args.n_cov_prior, args.n_steps).mc_episode(mean_prior, cov_prior, args.n_steps)\n",
    "            traces[run] = (step, Q)    \n",
    "                    \n",
    "            print('Run {} episode {} steps {} cumreward {} avg. duration {} switches {}'.format(run, episode, step, cumreward, avgduration, option_switches))\n",
    "        np.save(fname, traces)\n",
    "        dill.dump({'intra_policies':option_policies, 'policy':policy, 'term':option_terminations}, open('oc-options.pl', 'wb'))\n",
    "        print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[ 0.,  0.],\n",
       "        [ 0.,  0.]]), 1: array([[ 0.,  0.],\n",
       "        [ 0.,  0.]]), 2: array([[ 0.,  0.],\n",
       "        [ 0.,  0.]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = {k:np.zeros([2,2]) for k in range(3)}\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.random.uniform(0, 4, 3) for _ in range(2)]\n",
    "#x\n",
    "#c = np.mean(x, axis = 0)\n",
    "#c\n",
    "x_avg = np.mean(x,axis = 1)\n",
    "C = np.diag(x_avg)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = pm.Normal('sigmas', mu=0.1, tau=1000, size=2)\n",
    "centers = pm.Normal('centers', [0.3, 0.7], [1/(0.1)**2, 1/(0.1)**2], size=2)\n",
    "alpha  = pm.Beta('alpha', alpha=2, beta=3)\n",
    "category = pm.Container([pm.Categorical(\"category%i\" % i, [alpha, 1 - alpha]) \n",
    "                         for i in range(nsamples)])\n",
    "observations = pm.Container([pm.Normal('samples_model%i' % i, \n",
    "                   mu=centers[category[i]], tau=1/(sigmas[category[i]]**2), \n",
    "                   value=samples[i], observed=True) for i in range(nsamples)])\n",
    "model = pm.Model([observations, category, alpha, sigmas, centers])\n",
    "mcmc = pm.MCMC(model)\n",
    "# initialize in a good place to reduce the number of steps required\n",
    "centers.value = [mu1_true, mu2_true]\n",
    "# set a custom proposal for centers, since the default is bad\n",
    "mcmc.use_step_method(pm.Metropolis, centers, proposal_sd=sig1_true/np.sqrt(nsamples))\n",
    "# set a custom proposal for category, since the default is bad\n",
    "for i in range(nsamples):\n",
    "    mcmc.use_step_method(pm.DiscreteMetropolis, category[i], proposal_distribution='Prior')\n",
    "mcmc.sample(100)  # beware sampling takes much longer now\n",
    "# check the acceptance rates\n",
    "print(mcmc.step_method_dict[category[0]][0].ratio)\n",
    "print(mcmc.step_method_dict[centers][0].ratio)\n",
    "print(mcmc.step_method_dict[alpha][0].ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
