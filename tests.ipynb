{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "\n",
    "from distributed.belief import MultinomialDirichletBelief\n",
    "from distributed.broadcast import Broadcast\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DOC:\n",
    "    def __init__(self, env, options, mu_policy):\n",
    "        '''\n",
    "        :param states_list: all combination of joint states. This is an input from the environment\n",
    "        :param lr_thea: list of learning rates for learning policy parameters (pi), for all the agents\n",
    "        :param lr_phi: list of learning rates for learning termination functions (beta), for all the agents\n",
    "        :param init_observation: list of joint observation of all the agents\n",
    "        '''\n",
    "\n",
    "        self.env = env\n",
    "        self.options = options\n",
    "    \n",
    "        '''\n",
    "        2. Start with initial common belief b\n",
    "        '''\n",
    "        # set initial belief\n",
    "        initial_joint_observation = params['env']['initial_joint_state']\n",
    "        self.b = MultinomialDirichletBelief(env, initial_joint_observation)\n",
    "        #self.b0 = Belief(env)\n",
    "\n",
    "        '''\n",
    "        3. Sample a joint state s := vec(s_1,...,s_n) according to b_0\n",
    "        '''\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # policy over options\n",
    "        self.mu_policy = mu_policy\n",
    "\n",
    "        self.o = self.chooseOption()\n",
    "        self.a = self.chooseAction()\n",
    "\n",
    "        \n",
    "    def chooseOption(self):\n",
    "        # Choose joint-option o based on softmax option-policy\n",
    "        \n",
    "        #select agents randomly to pick options\n",
    "        agent_order = [agent.ID for agent in self.env.agents]\n",
    "        shuffle(agent_order)\n",
    "        print('agent order :', agent_order)\n",
    "    \n",
    "        #let agents select options from available option pool\n",
    "        for agent in agent_order:\n",
    "            option_mask = [not(option.available) for option in self.options]\n",
    "            # print(option_mask)\n",
    "    \n",
    "            # pmf = [0, 0, 0.7, 0.1, 0.2]\n",
    "            pmf = self.mu_policy.pmf(self.s[agent])\n",
    "            pmf = np.ma.masked_array(pmf, option_mask)\n",
    "            # print('pmf : ', pmf)\n",
    "\n",
    "            # select option for agent\n",
    "            # TODO : in order to sample option instead of choosing the best one, the masked pdf needs to be re-normalized\n",
    "            selected_option_idx = np.argmax(pmf)\n",
    "            self.env.agents[agent].option = self.options[selected_option_idx].optionID\n",
    "            # print(selected_option_idx)\n",
    "\n",
    "            #remove the selected option from available option pool by setting availability to False\n",
    "            self.options[selected_option_idx].available = False\n",
    "\n",
    "    def chooseAction(self):\n",
    "        joint_action = []\n",
    "        for agent in self.env.agents:\n",
    "            action = self.options[agent.option].policy.sample(agent.state)\n",
    "            print('agent ID:', agent.ID, 'state:', agent.state, 'option ID:', agent.option, 'action:', action)\n",
    "            joint_action.append(action)\n",
    "\n",
    "        return joint_action\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def evaluateOption(self, critic, action_critic, terminations, baseline=False):\n",
    "        # critic.start(joint_state, joint_option)\n",
    "        # action_critic.start(joint_state, joint_option, joint_action)\n",
    "        \n",
    "        reward, next_true_joint_state, done, _ = self.env.step(joint_action)\n",
    "\n",
    "        broadcasts = Broadcast(self.env, next_true_joint_state, self.s, self.o, terminations)\n",
    "\n",
    "        #broadcasts = self.env.broadcast(reward, next_true_joint_state, self.s, self.o, terminations)\n",
    "        joint_observation = self.env.get_observation(broadcasts)\n",
    "\n",
    "        self.b = MultinomialDirichletBelief(self.env, joint_observation)\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # Critic update\n",
    "        update_target = critic.update(self.s, self.o, reward, done)\n",
    "        action_critic.update(self.s, self.o, self.a, reward, done)\n",
    "\n",
    "\n",
    "        critic_feedback = action_critic.getQvalue(self.s, self.o, self.a)  #Q(s,o,a)\n",
    "\n",
    "        if baseline:\n",
    "            critic_feedback -= critic.value(self.s, self.o)\n",
    "        return critic_feedback\n",
    "\n",
    "\n",
    "    def improveOption_of_agent(self, agentID, intra_option_policy_improvement, termination_improvement, critic_feedback):\n",
    "        return intra_option_policy_improvement.update(agent_state, agent_action, critic_feedback), termination_improvement.update(agentID, self.s, self.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent order : [1, 2, 0]\n",
      "agent ID: 0 state: 39 option ID: 2 action: 1\n",
      "agent ID: 1 state: 35 option ID: 0 action: 3\n",
      "agent ID: 2 state: 66 option ID: 1 action: 3\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-43349bfc3991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mjoint_option\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDOC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavail_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0magent_optionID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_option\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magentID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mpi_policy_of_agent_option\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_optionID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0magent_option_termination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moption_terminations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_optionID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from fourroomsEnv import FourroomsMA\n",
    "from modelConfig import params\n",
    "from optionCritic.option import Option, createOptions\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "from optionCritic.termination import SigmoidTermination\n",
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "import optionCritic.gradients as grads\n",
    "\n",
    "env = FourroomsMA()\n",
    "avail_options, mu_policies = createOptions(env)\n",
    "\n",
    "joint_state_list = set([tuple(np.sort(s)) for s in env.states_list])\n",
    "joint_option_list = list(itertools.permutations(range(params['agent']['n_options']), params['env']['n_agents']))\n",
    "joint_action_list = list(itertools.product(range(len(env.agent_actions)), repeat=params['env']['n_agents']))\n",
    "\n",
    "# mu_policy is the policy over options\n",
    "mu_weights = dict.fromkeys(joint_state_list, dict.fromkeys(joint_option_list, 0))\n",
    "mu_policy = SoftmaxOptionPolicy(mu_weights)\n",
    "\n",
    "\n",
    "pi_policies = [SoftmaxActionPolicy(len(env.cell_list), len(env.agent_actions)) for _ in avail_options]\n",
    "\n",
    "# terminations take agent's state (not joint-state)\n",
    "option_terminations = [SigmoidTermination(len(env.cell_list)) for _ in range(params['agent']['n_options'])]\n",
    "critic = IntraOptionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations)\n",
    "\n",
    "\n",
    "action_critic = IntraOptionActionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations, critic)\n",
    "\n",
    "\n",
    "agent = DOC(env, avail_options, mu_policies).env.agents[0]\n",
    "print(joint_option)\n",
    "\n",
    "pi_policy_of_agent_option = pi_policies[agent.option]\n",
    "agent_option_termination = option_terminations[agen.option]\n",
    "\n",
    "intra_option_policy_improvement = grads.IntraOptionGradient(pi_policy_of_agent_option, params['doc']['lr_theta'])\n",
    "termination_improvement = grads.TerminationGradient(agent_option_termination, critic, params['doc']['lr_phi'])\n",
    "\n",
    "\n",
    "joint_state = DOC(env, avail_options, mu_policies).s\n",
    "# joint_option = (1,3,2)\n",
    "joint_action = DOC(env, avail_options, mu_policies).a\n",
    "\n",
    "# agent_state = joint_state[agentID]\n",
    "# agent_option = joint_option[agentID]\n",
    "# agent_action = joint_action[agentID]\n",
    "agent_state = agent.state\n",
    "agent_option = agent.option\n",
    "agent_action = agent.action\n",
    "\n",
    "critic.start(joint_state, joint_option)\n",
    "action_critic.start(joint_state, joint_option, joint_action)\n",
    "\n",
    "evalOption = DOC(env, avail_options, mu_policies).evaluateOption(critic, action_critic, option_terminations, baseline=False)\n",
    "imprvOption = DOC(env, avail_options, mu_policies).improveOption_of_agent(agent.ID, intra_option_policy_improvement, termination_improvement, evalOption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
