{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "\n",
    "from distributed.belief import MultinomialDirichletBelief\n",
    "from distributed.broadcast import Broadcast\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DOC:\n",
    "    def __init__(self, env, options, mu_policy):\n",
    "        '''\n",
    "        :param states_list: all combination of joint states. This is an input from the environment\n",
    "        :param lr_thea: list of learning rates for learning policy parameters (pi), for all the agents\n",
    "        :param lr_phi: list of learning rates for learning termination functions (beta), for all the agents\n",
    "        :param init_observation: list of joint observation of all the agents\n",
    "        '''\n",
    "\n",
    "        self.env = env\n",
    "        self.options = options\n",
    "    \n",
    "        '''\n",
    "        2. Start with initial common belief b\n",
    "        '''\n",
    "        # set initial belief\n",
    "        initial_joint_observation = params['env']['initial_joint_state']\n",
    "        self.b = MultinomialDirichletBelief(env, initial_joint_observation)\n",
    "        #self.b0 = Belief(env)\n",
    "\n",
    "        '''\n",
    "        3. Sample a joint state s := vec(s_1,...,s_n) according to b_0\n",
    "        '''\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # policy over options\n",
    "        self.mu_policy = mu_policy\n",
    "\n",
    "        self.o = self.chooseOption()\n",
    "        self.a = self.chooseAction()\n",
    "\n",
    "        \n",
    "    def chooseOption(self):\n",
    "        policy = creat\n",
    "#         # Choose joint-option o based on softmax option-policy\n",
    "        \n",
    "#         #select agents randomly to pick options\n",
    "#         agent_order = [agent.ID for agent in self.env.agents]\n",
    "#         shuffle(agent_order)\n",
    "#         print('agent order :', agent_order)\n",
    "    \n",
    "#         #let agents select options from available option pool\n",
    "#         for agent in agent_order:\n",
    "#             option_mask = [not(option.available) for option in self.options]\n",
    "#             # print(option_mask)\n",
    "    \n",
    "#             # pmf = [0, 0, 0.7, 0.1, 0.2]\n",
    "#             pmf = self.mu_policy.pmf(self.s[agent])\n",
    "#             pmf = np.ma.masked_array(pmf, option_mask)\n",
    "#             # print('pmf : ', pmf)\n",
    "\n",
    "#             # select option for agent\n",
    "#             # TODO : in order to sample option instead of choosing the best one, the masked pdf needs to be re-normalized\n",
    "#             selected_option_idx = np.argmax(pmf)\n",
    "#             self.env.agents[agent].option = self.options[selected_option_idx].optionID\n",
    "#             # print(selected_option_idx)\n",
    "\n",
    "#             #remove the selected option from available option pool by setting availability to False\n",
    "#             self.options[selected_option_idx].available = False\n",
    "\n",
    "    def chooseAction(self):\n",
    "        joint_action = []\n",
    "        for agent in self.env.agents:\n",
    "            action = self.options[agent.option].policy.sample(agent.state)\n",
    "            print('agent ID:', agent.ID, 'state:', agent.state, 'option ID:', agent.option, 'action:', action)\n",
    "            agent.action = action\n",
    "            joint_action.append(action)\n",
    "\n",
    "        return joint_action\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def evaluateOption(self, critic, action_critic, terminations, baseline=False):\n",
    "        # critic.start(joint_state, joint_option)\n",
    "        # action_critic.start(joint_state, joint_option, joint_action)\n",
    "        \n",
    "        reward, next_true_joint_state, done, _ = self.env.step(joint_action)\n",
    "\n",
    "        broadcasts = Broadcast(self.env, next_true_joint_state, self.s, self.o, terminations)\n",
    "\n",
    "        #broadcasts = self.env.broadcast(reward, next_true_joint_state, self.s, self.o, terminations)\n",
    "        joint_observation = self.env.get_observation(broadcasts)\n",
    "\n",
    "        self.b = MultinomialDirichletBelief(self.env, joint_observation)\n",
    "        self.s = self.b.sampleJointState()\n",
    "\n",
    "        # Critic update\n",
    "        update_target = critic.update(self.s, self.o, reward, done)\n",
    "        action_critic.update(self.s, self.o, self.a, reward, done)\n",
    "\n",
    "\n",
    "        critic_feedback = action_critic.getQvalue(self.s, self.o, self.a)  #Q(s,o,a)\n",
    "\n",
    "        if baseline:\n",
    "            critic_feedback -= critic.value(self.s, self.o)\n",
    "        return critic_feedback\n",
    "\n",
    "\n",
    "    def improveOption_of_agent(self, agentID, intra_option_policy_improvement, termination_improvement, critic_feedback):\n",
    "        return intra_option_policy_improvement.update(agent_state, agent_action, critic_feedback), termination_improvement.update(agentID, self.s, self.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent order : [0, 1, 2]\n",
      "agent ID: 0 state: 39 option ID: 0 action: 0\n",
      "agent ID: 1 state: 35 option ID: 1 action: 2\n",
      "agent ID: 2 state: 66 option ID: 2 action: 1\n",
      "agent <agent.Agent object at 0x111ab97f0> agent option 0\n",
      "agent order : [0, 1, 2]\n",
      "agent ID: 0 state: 39 option ID: 3 action: 3\n",
      "agent ID: 1 state: 35 option ID: 4 action: 3\n",
      "agent ID: 2 state: 66 option ID: 0 action: 1\n",
      "agent order : [0, 2, 1]\n",
      "agent ID: 0 state: 39 option ID: 0 action: 1\n",
      "agent ID: 1 state: 35 option ID: 0 action: 3\n",
      "agent ID: 2 state: 66 option ID: 0 action: 3\n",
      "jointoption None\n",
      "agent order : [2, 1, 0]\n",
      "agent ID: 0 state: 39 option ID: 0 action: 3\n",
      "agent ID: 1 state: 35 option ID: 0 action: 1\n",
      "agent ID: 2 state: 66 option ID: 0 action: 2\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis -1 is out of bounds for array of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ae6e172aa3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0magent_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0maction_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/simulations/teamOptionFourooms/optionCritic/Qlearning.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, joint_state, joint_option)\u001b[0m\n\u001b[1;32m     22\u001b[0m \t\t'''\n\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_joint_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_joint_option\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis -1 is out of bounds for array of dimension 0"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from fourroomsEnv import FourroomsMA\n",
    "from modelConfig import params\n",
    "from optionCritic.option import Option, createOptions\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "from optionCritic.termination import SigmoidTermination\n",
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "import optionCritic.gradients as grads\n",
    "\n",
    "env = FourroomsMA()\n",
    "avail_options, mu_policies = createOptions(env)\n",
    "\n",
    "joint_state_list = set([tuple(np.sort(s)) for s in env.states_list])\n",
    "joint_option_list = list(itertools.permutations(range(params['agent']['n_options']), params['env']['n_agents']))\n",
    "joint_action_list = list(itertools.product(range(len(env.agent_actions)), repeat=params['env']['n_agents']))\n",
    "\n",
    "# mu_policy is the policy over options\n",
    "mu_weights = dict.fromkeys(joint_state_list, dict.fromkeys(joint_option_list, 0))\n",
    "mu_policy = SoftmaxOptionPolicy(mu_weights)\n",
    "\n",
    "\n",
    "pi_policies = [SoftmaxActionPolicy(len(env.cell_list), len(env.agent_actions)) for _ in avail_options]\n",
    "\n",
    "# terminations take agent's state (not joint-state)\n",
    "option_terminations = [SigmoidTermination(len(env.cell_list)) for _ in range(params['agent']['n_options'])]\n",
    "critic = IntraOptionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations)\n",
    "\n",
    "\n",
    "action_critic = IntraOptionActionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations, critic)\n",
    "\n",
    "\n",
    "agent = DOC(env, avail_options, mu_policies).env.agents[0]\n",
    "print('agent',agent, 'agent option', agent.option)\n",
    "\n",
    "pi_policy_of_agent_option = pi_policies[agent.option]\n",
    "agent_option_termination = option_terminations[agent.option]\n",
    "\n",
    "intra_option_policy_improvement = grads.IntraOptionGradient(pi_policy_of_agent_option, params['doc']['lr_theta'])\n",
    "termination_improvement = grads.TerminationGradient(agent_option_termination, critic, params['doc']['lr_phi'])\n",
    "\n",
    "\n",
    "joint_state = DOC(env, avail_options, mu_policies).s\n",
    "joint_option = DOC(env, avail_options, mu_policies).o\n",
    "print('jointoption', joint_option)\n",
    "joint_action = DOC(env, avail_options, mu_policies).a\n",
    "\n",
    "# agent_state = joint_state[agentID]\n",
    "# agent_option = joint_option[agentID]\n",
    "# agent_action = joint_action[agentID]\n",
    "agent_state = agent.state\n",
    "agent_option = agent.option\n",
    "agent_action = agent.action\n",
    "\n",
    "critic.start(joint_state, joint_option)\n",
    "action_critic.start(joint_state, joint_option, joint_action)\n",
    "\n",
    "evalOption = DOC(env, avail_options, mu_policies).evaluateOption(critic, action_critic, option_terminations, baseline=False)\n",
    "imprvOption = DOC(env, avail_options, mu_policies).improveOption_of_agent(agent.ID, intra_option_policy_improvement, termination_improvement, evalOption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
