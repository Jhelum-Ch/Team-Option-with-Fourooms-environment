{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "\n",
    "from distributed.belief import MultinomialDirichletBelief\n",
    "from distributed.broadcast import Broadcast\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DOC:\n",
    "    def __init__(self, env, options, mu_policy):\n",
    "        '''\n",
    "        :param states_list: all combination of joint states. This is an input from the environment\n",
    "        :param lr_thea: list of learning rates for learning policy parameters (pi), for all the agents\n",
    "        :param lr_phi: list of learning rates for learning termination functions (beta), for all the agents\n",
    "        :param init_observation: list of joint observation of all the agents\n",
    "        '''\n",
    "\n",
    "        self.env = env\n",
    "        self.options = options\n",
    "    \n",
    "        '''\n",
    "        2. Start with initial common belief b\n",
    "        '''\n",
    "        # set initial belief\n",
    "        initial_joint_observation = params['env']['initial_joint_state']\n",
    "        self.belief = MultinomialDirichletBelief(env, initial_joint_observation)\n",
    "        #self.b0 = Belief(env)\n",
    "\n",
    "        '''\n",
    "        3. Sample a joint state s := vec(s_1,...,s_n) according to b_0\n",
    "        '''\n",
    "        self.joint_state = self.belief.sampleJointState()\n",
    "        print('joint_state',self.joint_state)\n",
    "\n",
    "        # policy over options\n",
    "        self.mu_policy = mu_policy\n",
    "        #print(self.mu_policy)\n",
    "\n",
    "        self.joint_option = self.chooseOption() #self.chooseOption()\n",
    "        self.joint_action = self.chooseAction(self.joint_state,self.joint_option)\n",
    "\n",
    "\n",
    "    def chooseOption(self):\n",
    "        # Choose joint-option o based on softmax option-policy\n",
    "        joint_state = tuple(np.sort(self.joint_state))\n",
    "\n",
    "        joint_option = self.mu_policy.sample(joint_state)\n",
    "        print('joint_option',joint_option)\n",
    "\n",
    "        for option in self.options:\n",
    "            option.available = True\n",
    "\n",
    "        for option in joint_option:\n",
    "            self.options[option].available = False\n",
    "\n",
    "        return joint_option\n",
    "\n",
    "#     def chooseAction(self):\n",
    "#         joint_action = []\n",
    "#         for agent in self.env.agents:\n",
    "#             print('agent state', agent.state, 'agent option', agent.option)\n",
    "#             action = self.options[agent.option].policy.sample(agent.state)\n",
    "#             print('agent ID:', agent.ID, 'state:', agent.state, 'option ID:', agent.option, 'action:', action)\n",
    "#             agent.action = action\n",
    "#             joint_action.append(action)\n",
    "\n",
    "#         return joint_action\n",
    "\n",
    "    def chooseAction(self, joint_state, joint_option):\n",
    "        joint_action = []\n",
    "        for agent in self.env.agents:\n",
    "            print('agent state', agent.state, 'agent option', agent.option)\n",
    "            agent.state = joint_state[agent.ID]\n",
    "            agent.option = joint_option[agent.ID]\n",
    "            agent_action = self.options[agent.option].policy.sample(agent.state)\n",
    "            print('agent ID:', agent.ID, 'state:', agent.state, 'option ID:', agent.option, 'agent action:', agent_action)\n",
    "            agent.action = agent_action\n",
    "            joint_action.append(agent_action)\n",
    "\n",
    "        return joint_action\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def evaluateOption(self, critic, action_critic, terminations, baseline=False):\n",
    "        # critic.start(joint_state, joint_option)\n",
    "        # action_critic.start(joint_state, joint_option, joint_action)\n",
    "        \n",
    "        reward, next_true_joint_state, done, _ = self.env.step(joint_action)\n",
    "\n",
    "        broadcasts = Broadcast(self.env, next_true_joint_state, self.joint_state, self.joint_option,done).broadcastBasedOnQ(critic,reward)\n",
    "\n",
    "        #broadcasts = self.env.broadcast(reward, next_true_joint_state, self.s, self.o, terminations)\n",
    "        joint_observation = self.env.get_observation(broadcasts)\n",
    "\n",
    "        self.belief = MultinomialDirichletBelief(self.env, joint_observation)\n",
    "        self.joint_state = self.belief.sampleJointState()\n",
    "\n",
    "        # Critic update\n",
    "        update_target = critic.update(self.joint_state, self.joint_option, reward, done)\n",
    "        action_critic.update(self.joint_state, self.joint_option, self.joint_action, reward, done)\n",
    "\n",
    "\n",
    "        critic_feedback = action_critic.getQvalue(self.joint_state, self.joint_option, self.joint_action)  #Q(s,o,a)\n",
    "\n",
    "        if baseline:\n",
    "            critic_feedback -= critic.value(self.joint_state, self.joint_option)\n",
    "        return critic_feedback\n",
    "\n",
    "\n",
    "    def improveOption_of_agent(self, agentID, intra_option_policy_improvement, termination_improvement, critic_feedback):\n",
    "        return intra_option_policy_improvement.update(agent_state, agent_action, critic_feedback), termination_improvement.update(agentID, self.joint_state, self.joint_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint_state (55, 50, 34)\n",
      "joint_option (0, 4, 3)\n",
      "agent state 39 agent option None\n",
      "agent ID: 0 state: 55 option ID: 0 agent action: 2\n",
      "agent state 35 agent option None\n",
      "agent ID: 1 state: 50 option ID: 4 agent action: 1\n",
      "agent state 66 agent option None\n",
      "agent ID: 2 state: 34 option ID: 3 agent action: 3\n",
      "joint state (55, 50, 34)\n",
      "joint_state (32, 81, 91)\n",
      "joint_option (3, 4, 1)\n",
      "agent state 55 agent option 0\n",
      "agent ID: 0 state: 32 option ID: 3 agent action: 1\n",
      "agent state 50 agent option 4\n",
      "agent ID: 1 state: 81 option ID: 4 agent action: 1\n",
      "agent state 34 agent option 3\n",
      "agent ID: 2 state: 91 option ID: 1 agent action: 3\n",
      "joint option (3, 4, 1)\n",
      "joint_state (6, 90, 80)\n",
      "joint_option (4, 3, 0)\n",
      "agent state 32 agent option 3\n",
      "agent ID: 0 state: 6 option ID: 4 agent action: 3\n",
      "agent state 81 agent option 4\n",
      "agent ID: 1 state: 90 option ID: 3 agent action: 1\n",
      "agent state 91 agent option 1\n",
      "agent ID: 2 state: 80 option ID: 0 agent action: 2\n",
      "agent1 <agent.Agent object at 0x111a782e8> agent1 option 4\n",
      "joint_state (39, 43, 46)\n",
      "joint_option (3, 1, 4)\n",
      "agent state 6 agent option 4\n",
      "agent ID: 0 state: 39 option ID: 3 agent action: 2\n",
      "agent state 90 agent option 3\n",
      "agent ID: 1 state: 43 option ID: 1 agent action: 1\n",
      "agent state 80 agent option 0\n",
      "agent ID: 2 state: 46 option ID: 4 agent action: 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(40, 43, 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f6c0db1e5d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0maction_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mevalOption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDOC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavail_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluateOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption_terminations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mimprvOption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDOC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavail_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimproveOption_of_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintra_option_policy_improvement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtermination_improvement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalOption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ca3516ee14a8>\u001b[0m in \u001b[0;36mevaluateOption\u001b[0;34m(self, critic, action_critic, terminations, baseline)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_true_joint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mbroadcasts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_true_joint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcastBasedOnQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m#broadcasts = self.env.broadcast(reward, next_true_joint_state, self.s, self.o, terminations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/simulations/teamOptionFourooms/distributed/broadcast.py\u001b[0m in \u001b[0;36mbroadcastBasedOnQ\u001b[0;34m(self, critic, reward)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#critic = IntraOptionQLearning(params['env']['discount'], params['doc']['lr_Q'],self.terminations, option_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcritic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mQ_agent_with_broadcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_current_joint_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# q1 = critic1.update(modified_current_joint_state, self.joint_option, reward+self.env.broadcast_penalty, self.done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Q_agent_with_broadcast = q1.getQvalue(modified_current_joint_state, None, self.joint_option)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/simulations/teamOptionFourooms/optionCritic/Qlearning.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, joint_state, joint_option, reward, done)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# Dense gradient update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mtderror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_joint_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_joint_option\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtderror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mupdate_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (40, 43, 46)"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from fourroomsEnv import FourroomsMA\n",
    "from modelConfig import params\n",
    "from optionCritic.option import Option, createOptions\n",
    "from optionCritic.policies import SoftmaxOptionPolicy, SoftmaxActionPolicy\n",
    "from optionCritic.termination import SigmoidTermination\n",
    "from optionCritic.Qlearning import IntraOptionQLearning, IntraOptionActionQLearning\n",
    "import optionCritic.gradients as grads\n",
    "\n",
    "env = FourroomsMA()\n",
    "avail_options, mu_policies = createOptions(env)\n",
    "#print(avail_options[0].optionID, avail_options[0].policy, avail_options[0].termination, avail_options[0].available)\n",
    "\n",
    "joint_state_list = set([tuple(np.sort(s)) for s in env.states_list])\n",
    "joint_option_list = list(itertools.permutations(range(params['agent']['n_options']), params['env']['n_agents']))\n",
    "# joint_action_list = list(itertools.product(range(len(env.agent_actions)), repeat=params['env']['n_agents']))\n",
    "\n",
    "# mu_policy is the policy over options\n",
    "mu_weights = dict.fromkeys(joint_state_list, dict.fromkeys(joint_option_list, 0))\n",
    "# mu_policy = SoftmaxOptionPolicy(mu_weights)\n",
    "\n",
    "\n",
    "pi_policies = [SoftmaxActionPolicy(len(env.cell_list), len(env.agent_actions)) for _ in avail_options]\n",
    "\n",
    "# terminations take agent's state (not joint-state)\n",
    "option_terminations = [SigmoidTermination(len(env.cell_list)) for _ in range(params['agent']['n_options'])]\n",
    "critic = IntraOptionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations, SoftmaxOptionPolicy(mu_weights).weights)\n",
    "\n",
    "\n",
    "action_critic = IntraOptionActionQLearning(params['env']['discount'], params['doc']['lr_Q'], option_terminations, SoftmaxActionPolicy(len(env.cell_list), len(env.agent_actions)).weights, critic)\n",
    "\n",
    "joint_state = DOC(env, avail_options, mu_policies).joint_state\n",
    "print('joint state', joint_state)\n",
    "joint_option = DOC(env, avail_options, mu_policies).joint_option\n",
    "print('joint option', joint_option)\n",
    "# for agent in env.agents:\n",
    "#     print('agent ID', agent.ID)\n",
    "#     agent.state = joint_state[agent.ID]\n",
    "#     agent.option = joint_option[agent.ID]\n",
    "    \n",
    "\n",
    "joint_action = DOC(env, avail_options, mu_policies).joint_action\n",
    "# for agent in env.agents:\n",
    "#     agent.action = joint_action[agent.ID]\n",
    "\n",
    "\n",
    "# test with Agent~1\n",
    "agent1 = env.agents[0]\n",
    "print('agent1',agent1, 'agent1 option', agent1.option)\n",
    "\n",
    "agent1_state = agent1.state\n",
    "agent1_option = agent1.option\n",
    "agent1_action = agent1.action\n",
    "\n",
    "pi_policy_of_agent_option = pi_policies[agent1_option]\n",
    "agent_option_termination = option_terminations[agent1_option]\n",
    "\n",
    "intra_option_policy_improvement = grads.IntraOptionGradient(pi_policy_of_agent_option, params['doc']['lr_theta'])\n",
    "termination_improvement = grads.TerminationGradient(agent_option_termination, critic, params['doc']['lr_phi'])\n",
    "\n",
    "\n",
    "\n",
    "critic.start(joint_state, joint_option)\n",
    "action_critic.start(joint_state, joint_option, joint_action)\n",
    "\n",
    "evalOption = DOC(env, avail_options, mu_policies).evaluateOption(critic, action_critic, option_terminations, baseline=False)\n",
    "imprvOption = DOC(env, avail_options, mu_policies).improveOption_of_agent(agent1.ID, intra_option_policy_improvement, termination_improvement, evalOption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
